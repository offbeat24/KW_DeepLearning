{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2016741011_박시현_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OA0f9oxYGYb"
      },
      "source": [
        "# HW#6 Regularization\n",
        "\n",
        "안녕하세요, 광운대학교 로봇학부의 오정현 교수입니다. 본 자료는 딥러닝 실습 수업을 위해 제작된 것입니다.\n",
        "\n",
        "파이썬 문법\n",
        "- 점프투파이썬(https://wikidocs.net/book/1) 참고\n",
        "\n",
        "이번 과제는 딥러닝의 일반화 성능을 높이기 위한 Regularization을 해보는 것입니다.이미지 분류에 여러 가지 Regularization 기법을 적용해 보도록 하겠습니다. 대표적인 Regularization 기법으로 Dropout, Data augmentation, Batch Normalization 등이 있습니다.\n",
        "\n",
        "이번 과제는 (https://www.tensorflow.org/tutorials/keras/classification?hl=ko)를 참고하면 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9KBzwzd7Bub"
      },
      "source": [
        "#1. Data Generation\n",
        "Data는 mnist dataset을 이용하도록 하겠습니다. mnist dataset은 원래 60000개의 training set이 주어져 있지만 overfitting을 유도하기 위하여 1000개의 data만 이용하려고 합니다. 1000개의 data로 이루어진 x_train과 y_train을 만들어 보세요. 그리고 2000개로 이루어진 large_x_train, large_y_train을 만들어보세요. 그리고 training data의 다른 범위에서 200개로 이루어진 x_validation과 y_validation도 만들어 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3K3NqD_wTyh",
        "outputId": "29302211-149b-486b-8d98-0424b620bee3"
      },
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow import keras \n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Flatten\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot\n",
        "\n",
        "batch_size = 28\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(X_train, Y_train), (x_test,y_test) = mnist.load_data()\n",
        "\n",
        "### START CODE HERE ###\n",
        "x_validation = X_train[:200][:][:]\n",
        "y_validation = Y_train[:200][:][:]\n",
        "large_x_train = X_train[200:2200][:][:]\n",
        "large_y_train = Y_train[200:2200][:][:]\n",
        "x_train = X_train[2200:3200][:][:]\n",
        "y_train = Y_train[2200:3200][:][:]\n",
        "### END CODE HERE ###\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "large_x_train = large_x_train.astype('float32')\n",
        "x_validation = x_validation.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "large_x_train /= 255\n",
        "x_validation /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "large_y_train = keras.utils.to_categorical(large_y_train, num_classes)\n",
        "y_validation = keras.utils.to_categorical(y_validation, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "assert large_x_train.shape[0]==2000\n",
        "assert large_y_train.shape[0]==2000\n",
        "assert x_train.shape[0]==1000\n",
        "assert y_train.shape[0]==1000\n",
        "assert x_validation.shape[0]==200\n",
        "assert y_validation.shape[0]==200\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"large_x_train shape:\", large_x_train.shape)\n",
        "print(\"large_y_train shape:\", large_y_train.shape)\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_validation shape: (200, 28, 28)\n",
            "y_validation shape: (200,)\n",
            "x_train shape: (1000, 28, 28, 1)\n",
            "y_train shape: (1000, 10)\n",
            "large_x_train shape: (2000, 28, 28, 1)\n",
            "large_y_train shape: (2000, 10)\n",
            "x_validation shape: (200, 28, 28, 1)\n",
            "y_validation shape: (200, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlNTST2K_IRE"
      },
      "source": [
        "#2. 모델 생성\n",
        "복습 차원에서 MLP 분류모델을 만들어 보도록 하겠습니다. 모델의 마지막 레이어에는 활성화 함수로 10개의 출력을 가지는 softmax를 달겠습니다. 이를 통해서 모델은 이미지안의 숫자가 0부터 9까지의 숫자중에 어디에 가까운지를 확률적으로 나타냅니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNI8KWo8PyC"
      },
      "source": [
        "다음과 같은 MLP 모델을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560L-utPx8z1",
        "outputId": "46d73dca-38da-429d-f10a-395167a3ad2d"
      },
      "source": [
        "model = Sequential()\n",
        "### START CODE HERE ###\n",
        "model.add(keras.Input(shape=(28,28)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(1024))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.build()\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_12 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhIfmZeBZm8"
      },
      "source": [
        "#3. Learning MLP\n",
        "기본 MLP 분류모델을 학습해 보겠습니다. Overfitting은 Training data에 맞추어 과도하게 학습이 이루어져 Test data에서 높은 성능이 나지 않는 현상, 즉 Generalization 성능이 높지 않게 나타나는 현상을 의미합니다. 따라서 Overfitting이 발생하면 Training accuracy는 높지만 Test accuracy는 높지 않게 나타납니다. \n",
        "\n",
        "현재 모델이 overfitting이 발생하는지 체크해 보세요. 일부러 overfitting이 발생하도록 유도하였기 때문에 overfitting 현상이 나타나야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1VoI2bCyBy-",
        "outputId": "434142ad-7092-4565-c1ee-52090d3db9b7"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "weights = model.get_weights()\n",
        "\n",
        "history=model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 2s 34ms/step - loss: 2.4013 - accuracy: 0.1030 - val_loss: 2.3379 - val_accuracy: 0.1350\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 2.3144 - accuracy: 0.1220 - val_loss: 2.2650 - val_accuracy: 0.1650\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 2.2339 - accuracy: 0.1550 - val_loss: 2.1967 - val_accuracy: 0.1900\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 2.1587 - accuracy: 0.2220 - val_loss: 2.1321 - val_accuracy: 0.2650\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 2.0869 - accuracy: 0.2940 - val_loss: 2.0706 - val_accuracy: 0.3350\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 2.0190 - accuracy: 0.3720 - val_loss: 2.0119 - val_accuracy: 0.3850\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.9541 - accuracy: 0.4390 - val_loss: 1.9555 - val_accuracy: 0.4300\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.8924 - accuracy: 0.4970 - val_loss: 1.9017 - val_accuracy: 0.4850\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 1.8337 - accuracy: 0.5390 - val_loss: 1.8504 - val_accuracy: 0.5150\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.7770 - accuracy: 0.5730 - val_loss: 1.8016 - val_accuracy: 0.5550\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.7235 - accuracy: 0.6070 - val_loss: 1.7544 - val_accuracy: 0.5800\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.6722 - accuracy: 0.6350 - val_loss: 1.7096 - val_accuracy: 0.5950\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.6229 - accuracy: 0.6610 - val_loss: 1.6660 - val_accuracy: 0.6000\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.5762 - accuracy: 0.6750 - val_loss: 1.6250 - val_accuracy: 0.6050\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.5312 - accuracy: 0.6880 - val_loss: 1.5850 - val_accuracy: 0.6350\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.4887 - accuracy: 0.6980 - val_loss: 1.5471 - val_accuracy: 0.6450\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.4478 - accuracy: 0.7160 - val_loss: 1.5110 - val_accuracy: 0.6600\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.4090 - accuracy: 0.7170 - val_loss: 1.4765 - val_accuracy: 0.6700\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.3718 - accuracy: 0.7300 - val_loss: 1.4435 - val_accuracy: 0.6850\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.3361 - accuracy: 0.7370 - val_loss: 1.4113 - val_accuracy: 0.7000\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.3022 - accuracy: 0.7420 - val_loss: 1.3805 - val_accuracy: 0.7150\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.2695 - accuracy: 0.7490 - val_loss: 1.3506 - val_accuracy: 0.7300\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.2385 - accuracy: 0.7580 - val_loss: 1.3223 - val_accuracy: 0.7350\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.2084 - accuracy: 0.7640 - val_loss: 1.2949 - val_accuracy: 0.7400\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.1799 - accuracy: 0.7660 - val_loss: 1.2685 - val_accuracy: 0.7600\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.1527 - accuracy: 0.7690 - val_loss: 1.2435 - val_accuracy: 0.7600\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.1267 - accuracy: 0.7730 - val_loss: 1.2195 - val_accuracy: 0.7700\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 1.1018 - accuracy: 0.7770 - val_loss: 1.1967 - val_accuracy: 0.7700\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 2s 48ms/step - loss: 1.0781 - accuracy: 0.7840 - val_loss: 1.1746 - val_accuracy: 0.7750\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 1.0553 - accuracy: 0.7890 - val_loss: 1.1532 - val_accuracy: 0.7750\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.0337 - accuracy: 0.7950 - val_loss: 1.1332 - val_accuracy: 0.7750\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 1.0130 - accuracy: 0.7960 - val_loss: 1.1142 - val_accuracy: 0.7750\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.9930 - accuracy: 0.8020 - val_loss: 1.0958 - val_accuracy: 0.7750\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.9741 - accuracy: 0.8080 - val_loss: 1.0781 - val_accuracy: 0.7750\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.9559 - accuracy: 0.8060 - val_loss: 1.0611 - val_accuracy: 0.7750\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.9384 - accuracy: 0.8090 - val_loss: 1.0445 - val_accuracy: 0.7750\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.9217 - accuracy: 0.8110 - val_loss: 1.0288 - val_accuracy: 0.7750\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.9055 - accuracy: 0.8130 - val_loss: 1.0133 - val_accuracy: 0.7850\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.8901 - accuracy: 0.8170 - val_loss: 0.9986 - val_accuracy: 0.7950\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 1s 31ms/step - loss: 0.8753 - accuracy: 0.8140 - val_loss: 0.9850 - val_accuracy: 0.7950\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.8609 - accuracy: 0.8180 - val_loss: 0.9713 - val_accuracy: 0.7950\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.8472 - accuracy: 0.8210 - val_loss: 0.9584 - val_accuracy: 0.7950\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 1s 31ms/step - loss: 0.8339 - accuracy: 0.8240 - val_loss: 0.9462 - val_accuracy: 0.7950\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.8212 - accuracy: 0.8240 - val_loss: 0.9342 - val_accuracy: 0.8000\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.8088 - accuracy: 0.8260 - val_loss: 0.9227 - val_accuracy: 0.8000\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.7971 - accuracy: 0.8270 - val_loss: 0.9116 - val_accuracy: 0.8000\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.7856 - accuracy: 0.8280 - val_loss: 0.9008 - val_accuracy: 0.7950\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.7746 - accuracy: 0.8330 - val_loss: 0.8902 - val_accuracy: 0.7950\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.7640 - accuracy: 0.8310 - val_loss: 0.8801 - val_accuracy: 0.7950\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.7538 - accuracy: 0.8350 - val_loss: 0.8703 - val_accuracy: 0.7900\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.7440 - accuracy: 0.8370 - val_loss: 0.8609 - val_accuracy: 0.8000\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.7345 - accuracy: 0.8390 - val_loss: 0.8519 - val_accuracy: 0.8000\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 1s 31ms/step - loss: 0.7251 - accuracy: 0.8400 - val_loss: 0.8430 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.7163 - accuracy: 0.8400 - val_loss: 0.8345 - val_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.7076 - accuracy: 0.8400 - val_loss: 0.8262 - val_accuracy: 0.8000\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.6992 - accuracy: 0.8430 - val_loss: 0.8179 - val_accuracy: 0.8000\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.6911 - accuracy: 0.8430 - val_loss: 0.8100 - val_accuracy: 0.7950\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.6832 - accuracy: 0.8470 - val_loss: 0.8028 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.6755 - accuracy: 0.8480 - val_loss: 0.7955 - val_accuracy: 0.8000\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6682 - accuracy: 0.8510 - val_loss: 0.7882 - val_accuracy: 0.7950\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6609 - accuracy: 0.8510 - val_loss: 0.7808 - val_accuracy: 0.7950\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6538 - accuracy: 0.8520 - val_loss: 0.7740 - val_accuracy: 0.7950\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.6470 - accuracy: 0.8540 - val_loss: 0.7675 - val_accuracy: 0.7950\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6403 - accuracy: 0.8550 - val_loss: 0.7611 - val_accuracy: 0.8000\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6337 - accuracy: 0.8570 - val_loss: 0.7548 - val_accuracy: 0.8000\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6274 - accuracy: 0.8580 - val_loss: 0.7489 - val_accuracy: 0.8000\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6213 - accuracy: 0.8580 - val_loss: 0.7428 - val_accuracy: 0.8000\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.6153 - accuracy: 0.8590 - val_loss: 0.7372 - val_accuracy: 0.8000\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 1s 30ms/step - loss: 0.6095 - accuracy: 0.8590 - val_loss: 0.7319 - val_accuracy: 0.8000\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.6038 - accuracy: 0.8590 - val_loss: 0.7264 - val_accuracy: 0.8000\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5983 - accuracy: 0.8600 - val_loss: 0.7209 - val_accuracy: 0.8000\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5929 - accuracy: 0.8620 - val_loss: 0.7157 - val_accuracy: 0.8000\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5877 - accuracy: 0.8640 - val_loss: 0.7104 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5826 - accuracy: 0.8660 - val_loss: 0.7055 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5777 - accuracy: 0.8660 - val_loss: 0.7009 - val_accuracy: 0.8000\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5727 - accuracy: 0.8690 - val_loss: 0.6961 - val_accuracy: 0.8100\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5680 - accuracy: 0.8700 - val_loss: 0.6918 - val_accuracy: 0.8100\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5634 - accuracy: 0.8700 - val_loss: 0.6874 - val_accuracy: 0.8100\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5588 - accuracy: 0.8720 - val_loss: 0.6832 - val_accuracy: 0.8100\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5544 - accuracy: 0.8710 - val_loss: 0.6791 - val_accuracy: 0.8100\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5500 - accuracy: 0.8700 - val_loss: 0.6751 - val_accuracy: 0.8100\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5457 - accuracy: 0.8740 - val_loss: 0.6707 - val_accuracy: 0.8100\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5416 - accuracy: 0.8730 - val_loss: 0.6668 - val_accuracy: 0.8100\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5375 - accuracy: 0.8770 - val_loss: 0.6630 - val_accuracy: 0.8100\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5335 - accuracy: 0.8750 - val_loss: 0.6588 - val_accuracy: 0.8150\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5296 - accuracy: 0.8760 - val_loss: 0.6553 - val_accuracy: 0.8150\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5257 - accuracy: 0.8760 - val_loss: 0.6520 - val_accuracy: 0.8150\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5220 - accuracy: 0.8760 - val_loss: 0.6483 - val_accuracy: 0.8150\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5184 - accuracy: 0.8770 - val_loss: 0.6451 - val_accuracy: 0.8150\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5147 - accuracy: 0.8770 - val_loss: 0.6418 - val_accuracy: 0.8150\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5112 - accuracy: 0.8780 - val_loss: 0.6389 - val_accuracy: 0.8150\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5077 - accuracy: 0.8780 - val_loss: 0.6356 - val_accuracy: 0.8150\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.5044 - accuracy: 0.8790 - val_loss: 0.6322 - val_accuracy: 0.8150\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.5010 - accuracy: 0.8800 - val_loss: 0.6291 - val_accuracy: 0.8150\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.4978 - accuracy: 0.8790 - val_loss: 0.6259 - val_accuracy: 0.8150\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.4945 - accuracy: 0.8800 - val_loss: 0.6230 - val_accuracy: 0.8150\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.4914 - accuracy: 0.8810 - val_loss: 0.6199 - val_accuracy: 0.8150\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.4883 - accuracy: 0.8830 - val_loss: 0.6169 - val_accuracy: 0.8200\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.4852 - accuracy: 0.8810 - val_loss: 0.6142 - val_accuracy: 0.8200\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.4822 - accuracy: 0.8820 - val_loss: 0.6115 - val_accuracy: 0.8250\n",
            "Test loss: 0.6034075021743774\n",
            "Test accuracy: 0.8289999961853027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jebNKzFBJGE"
      },
      "source": [
        "#4. Regularization\n",
        "Overfitting이 발생한 모델에 다양한 Regularization 기법을 이용해 보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxZKsjCFvhT"
      },
      "source": [
        "4.1 Large Dataset\n",
        "\n",
        "Training data가 충분하다면 overfitting 현상이 발생할 가능성이 줄어듭니다. 기본 MLP 분류 모델에서 large_x_train과 large_y_train을 이용하면 성능이 올라갈 것입니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5b1fp96FuEy",
        "outputId": "f6e68b32-1ec2-4bc1-99bf-4bcd9b0241a1"
      },
      "source": [
        "#initialize weights\n",
        "model.set_weights(weights)\n",
        "\n",
        "large_model_history=model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Largemodel Test loss:', score[0])\n",
        "print('Largemodel Test accuracy:', score[1])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 2.3114 - accuracy: 0.1380 - val_loss: 2.2054 - val_accuracy: 0.1950\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 2.1006 - accuracy: 0.2630 - val_loss: 2.0404 - val_accuracy: 0.3200\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.9303 - accuracy: 0.4490 - val_loss: 1.9013 - val_accuracy: 0.4650\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.7872 - accuracy: 0.5460 - val_loss: 1.7838 - val_accuracy: 0.5600\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.6646 - accuracy: 0.6100 - val_loss: 1.6807 - val_accuracy: 0.6350\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.5582 - accuracy: 0.6570 - val_loss: 1.5904 - val_accuracy: 0.6950\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.4648 - accuracy: 0.6885 - val_loss: 1.5107 - val_accuracy: 0.7050\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.3822 - accuracy: 0.7130 - val_loss: 1.4387 - val_accuracy: 0.7100\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.3087 - accuracy: 0.7320 - val_loss: 1.3735 - val_accuracy: 0.7300\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.2428 - accuracy: 0.7500 - val_loss: 1.3143 - val_accuracy: 0.7550\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.1833 - accuracy: 0.7665 - val_loss: 1.2609 - val_accuracy: 0.7800\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.1295 - accuracy: 0.7795 - val_loss: 1.2117 - val_accuracy: 0.7800\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.0804 - accuracy: 0.7885 - val_loss: 1.1657 - val_accuracy: 0.7850\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 1.0357 - accuracy: 0.7985 - val_loss: 1.1234 - val_accuracy: 0.7900\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.9948 - accuracy: 0.8025 - val_loss: 1.0858 - val_accuracy: 0.7950\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.9574 - accuracy: 0.8100 - val_loss: 1.0503 - val_accuracy: 0.7950\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.9230 - accuracy: 0.8155 - val_loss: 1.0177 - val_accuracy: 0.8000\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.8913 - accuracy: 0.8205 - val_loss: 0.9879 - val_accuracy: 0.8000\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8619 - accuracy: 0.8235 - val_loss: 0.9598 - val_accuracy: 0.8050\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8348 - accuracy: 0.8290 - val_loss: 0.9332 - val_accuracy: 0.8150\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8097 - accuracy: 0.8335 - val_loss: 0.9090 - val_accuracy: 0.8200\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7864 - accuracy: 0.8365 - val_loss: 0.8859 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7649 - accuracy: 0.8415 - val_loss: 0.8645 - val_accuracy: 0.8400\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7448 - accuracy: 0.8415 - val_loss: 0.8444 - val_accuracy: 0.8400\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7261 - accuracy: 0.8440 - val_loss: 0.8271 - val_accuracy: 0.8450\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7086 - accuracy: 0.8455 - val_loss: 0.8101 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6920 - accuracy: 0.8505 - val_loss: 0.7940 - val_accuracy: 0.8500\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6766 - accuracy: 0.8550 - val_loss: 0.7784 - val_accuracy: 0.8550\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6622 - accuracy: 0.8565 - val_loss: 0.7640 - val_accuracy: 0.8550\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6486 - accuracy: 0.8570 - val_loss: 0.7503 - val_accuracy: 0.8550\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6357 - accuracy: 0.8600 - val_loss: 0.7377 - val_accuracy: 0.8550\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6234 - accuracy: 0.8615 - val_loss: 0.7243 - val_accuracy: 0.8550\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6118 - accuracy: 0.8610 - val_loss: 0.7140 - val_accuracy: 0.8600\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6007 - accuracy: 0.8640 - val_loss: 0.7023 - val_accuracy: 0.8650\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.5901 - accuracy: 0.8665 - val_loss: 0.6921 - val_accuracy: 0.8600\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5801 - accuracy: 0.8665 - val_loss: 0.6829 - val_accuracy: 0.8600\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5706 - accuracy: 0.8685 - val_loss: 0.6732 - val_accuracy: 0.8600\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5616 - accuracy: 0.8685 - val_loss: 0.6636 - val_accuracy: 0.8650\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5530 - accuracy: 0.8695 - val_loss: 0.6553 - val_accuracy: 0.8700\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5448 - accuracy: 0.8720 - val_loss: 0.6466 - val_accuracy: 0.8700\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5371 - accuracy: 0.8720 - val_loss: 0.6382 - val_accuracy: 0.8700\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5296 - accuracy: 0.8725 - val_loss: 0.6310 - val_accuracy: 0.8700\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5224 - accuracy: 0.8735 - val_loss: 0.6233 - val_accuracy: 0.8700\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.5156 - accuracy: 0.8725 - val_loss: 0.6174 - val_accuracy: 0.8700\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5090 - accuracy: 0.8755 - val_loss: 0.6105 - val_accuracy: 0.8750\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5026 - accuracy: 0.8745 - val_loss: 0.6047 - val_accuracy: 0.8700\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4965 - accuracy: 0.8770 - val_loss: 0.5980 - val_accuracy: 0.8750\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4906 - accuracy: 0.8770 - val_loss: 0.5922 - val_accuracy: 0.8750\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4849 - accuracy: 0.8780 - val_loss: 0.5871 - val_accuracy: 0.8750\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4794 - accuracy: 0.8780 - val_loss: 0.5813 - val_accuracy: 0.8750\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4741 - accuracy: 0.8795 - val_loss: 0.5755 - val_accuracy: 0.8750\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4690 - accuracy: 0.8820 - val_loss: 0.5700 - val_accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4640 - accuracy: 0.8805 - val_loss: 0.5659 - val_accuracy: 0.8750\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4593 - accuracy: 0.8825 - val_loss: 0.5609 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4546 - accuracy: 0.8820 - val_loss: 0.5563 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4501 - accuracy: 0.8825 - val_loss: 0.5516 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.4458 - accuracy: 0.8850 - val_loss: 0.5474 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4414 - accuracy: 0.8835 - val_loss: 0.5434 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4373 - accuracy: 0.8860 - val_loss: 0.5394 - val_accuracy: 0.8750\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4334 - accuracy: 0.8860 - val_loss: 0.5355 - val_accuracy: 0.8750\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4296 - accuracy: 0.8875 - val_loss: 0.5317 - val_accuracy: 0.8750\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4258 - accuracy: 0.8870 - val_loss: 0.5285 - val_accuracy: 0.8800\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4221 - accuracy: 0.8900 - val_loss: 0.5247 - val_accuracy: 0.8800\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4186 - accuracy: 0.8905 - val_loss: 0.5215 - val_accuracy: 0.8800\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4151 - accuracy: 0.8920 - val_loss: 0.5183 - val_accuracy: 0.8800\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4117 - accuracy: 0.8930 - val_loss: 0.5158 - val_accuracy: 0.8800\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4084 - accuracy: 0.8930 - val_loss: 0.5122 - val_accuracy: 0.8800\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4052 - accuracy: 0.8945 - val_loss: 0.5086 - val_accuracy: 0.8900\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4021 - accuracy: 0.8960 - val_loss: 0.5053 - val_accuracy: 0.8950\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3990 - accuracy: 0.8970 - val_loss: 0.5027 - val_accuracy: 0.8950\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3960 - accuracy: 0.8980 - val_loss: 0.5002 - val_accuracy: 0.8950\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3932 - accuracy: 0.8985 - val_loss: 0.4976 - val_accuracy: 0.8950\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3904 - accuracy: 0.9000 - val_loss: 0.4951 - val_accuracy: 0.8950\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3877 - accuracy: 0.9000 - val_loss: 0.4925 - val_accuracy: 0.8950\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3849 - accuracy: 0.9010 - val_loss: 0.4894 - val_accuracy: 0.8950\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3823 - accuracy: 0.9005 - val_loss: 0.4871 - val_accuracy: 0.8950\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3797 - accuracy: 0.9000 - val_loss: 0.4849 - val_accuracy: 0.8950\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3771 - accuracy: 0.9025 - val_loss: 0.4829 - val_accuracy: 0.8950\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.3746 - accuracy: 0.9025 - val_loss: 0.4804 - val_accuracy: 0.8950\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3721 - accuracy: 0.9030 - val_loss: 0.4785 - val_accuracy: 0.8950\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3698 - accuracy: 0.9030 - val_loss: 0.4759 - val_accuracy: 0.8950\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3674 - accuracy: 0.9040 - val_loss: 0.4747 - val_accuracy: 0.8950\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3652 - accuracy: 0.9050 - val_loss: 0.4723 - val_accuracy: 0.8950\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3629 - accuracy: 0.9045 - val_loss: 0.4699 - val_accuracy: 0.8950\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3607 - accuracy: 0.9050 - val_loss: 0.4686 - val_accuracy: 0.8950\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3586 - accuracy: 0.9075 - val_loss: 0.4662 - val_accuracy: 0.8950\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3564 - accuracy: 0.9060 - val_loss: 0.4648 - val_accuracy: 0.8950\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3544 - accuracy: 0.9065 - val_loss: 0.4630 - val_accuracy: 0.8950\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3524 - accuracy: 0.9075 - val_loss: 0.4614 - val_accuracy: 0.8950\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3503 - accuracy: 0.9080 - val_loss: 0.4595 - val_accuracy: 0.8950\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3484 - accuracy: 0.9085 - val_loss: 0.4579 - val_accuracy: 0.8950\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.3464 - accuracy: 0.9080 - val_loss: 0.4559 - val_accuracy: 0.8950\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3446 - accuracy: 0.9095 - val_loss: 0.4539 - val_accuracy: 0.8950\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3427 - accuracy: 0.9100 - val_loss: 0.4517 - val_accuracy: 0.8950\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3409 - accuracy: 0.9100 - val_loss: 0.4499 - val_accuracy: 0.8950\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3391 - accuracy: 0.9115 - val_loss: 0.4482 - val_accuracy: 0.9000\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3373 - accuracy: 0.9105 - val_loss: 0.4476 - val_accuracy: 0.9000\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3356 - accuracy: 0.9105 - val_loss: 0.4455 - val_accuracy: 0.9000\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3339 - accuracy: 0.9120 - val_loss: 0.4443 - val_accuracy: 0.9000\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3322 - accuracy: 0.9125 - val_loss: 0.4431 - val_accuracy: 0.9000\n",
            "Largemodel Test loss: 0.43381449580192566\n",
            "Largemodel Test accuracy: 0.8810999989509583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j_-85YFDA6y"
      },
      "source": [
        "##4.2 Dropout\n",
        "Dropout은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 Dropout layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 Dropout model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYHpRcVDX22",
        "outputId": "87220f39-6624-4ea9-e49e-a1fd8e37c9be"
      },
      "source": [
        "dropout_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "dropout_model.add(keras.Input(shape=(28,28,)))\n",
        "dropout_model.add(Flatten())\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dropout(0.2))\n",
        "dropout_model.add(Flatten())\n",
        "dropout_model.add(Dense(10, activation=\"softmax\"))\n",
        "### END CODE HERE ###\n",
        "\n",
        "dropout_model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_18 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuxjOGWEKo9"
      },
      "source": [
        "Dropout Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?\n",
        "\n",
        "※Dropout을 적용한 Model은 일정확률로 신경망의 뉴런을 비활성화시키기 때문에, 오버피팅을 방지하는 효과가 있습니다. 하지만 비활성화로 인해서  학습속도가 떨어진다는 단점이 존재합니다. 그 때문에 test accuracy는 같은 학습파라미터 조건에서 기존모델보다 더 낮을 수 있습니다. \n",
        "\n",
        "기존모델과 dropout을 적용한 모델에 대해서, train 데이터와 test데이터에 대한 accuracy차이를 주목해보시면 좋을 것같습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqMK-jsyED_h",
        "outputId": "0016c808-54b2-438f-b9f4-c349c878594f"
      },
      "source": [
        "dropout_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dropout_model_history=dropout_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = dropout_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Dropout model Test loss:', score[0])\n",
        "print('Dropout model Test accuracy:', score[1])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 3s 30ms/step - loss: 2.4144 - accuracy: 0.1145 - val_loss: 2.3845 - val_accuracy: 0.0950\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 2.2481 - accuracy: 0.1695 - val_loss: 2.2514 - val_accuracy: 0.1600\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 2.1182 - accuracy: 0.2630 - val_loss: 2.1318 - val_accuracy: 0.2900\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.9806 - accuracy: 0.3990 - val_loss: 2.0234 - val_accuracy: 0.3850\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 1.8736 - accuracy: 0.4745 - val_loss: 1.9221 - val_accuracy: 0.4950\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.7736 - accuracy: 0.5520 - val_loss: 1.8294 - val_accuracy: 0.5600\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.6833 - accuracy: 0.6125 - val_loss: 1.7448 - val_accuracy: 0.6100\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.5916 - accuracy: 0.6570 - val_loss: 1.6659 - val_accuracy: 0.6450\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.5075 - accuracy: 0.6835 - val_loss: 1.5929 - val_accuracy: 0.6600\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.4332 - accuracy: 0.7040 - val_loss: 1.5262 - val_accuracy: 0.6700\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.3634 - accuracy: 0.7205 - val_loss: 1.4629 - val_accuracy: 0.6900\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.2993 - accuracy: 0.7335 - val_loss: 1.4056 - val_accuracy: 0.7150\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.2426 - accuracy: 0.7450 - val_loss: 1.3518 - val_accuracy: 0.7250\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.1852 - accuracy: 0.7630 - val_loss: 1.3025 - val_accuracy: 0.7300\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.1394 - accuracy: 0.7720 - val_loss: 1.2565 - val_accuracy: 0.7450\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.0966 - accuracy: 0.7740 - val_loss: 1.2120 - val_accuracy: 0.7600\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 1.0570 - accuracy: 0.7805 - val_loss: 1.1717 - val_accuracy: 0.7800\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 1.0140 - accuracy: 0.7885 - val_loss: 1.1358 - val_accuracy: 0.7800\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.9822 - accuracy: 0.7875 - val_loss: 1.1008 - val_accuracy: 0.7850\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.9453 - accuracy: 0.7985 - val_loss: 1.0696 - val_accuracy: 0.7800\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.9209 - accuracy: 0.8040 - val_loss: 1.0400 - val_accuracy: 0.7850\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8921 - accuracy: 0.8095 - val_loss: 1.0120 - val_accuracy: 0.7900\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.8569 - accuracy: 0.8240 - val_loss: 0.9857 - val_accuracy: 0.8000\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.8375 - accuracy: 0.8165 - val_loss: 0.9614 - val_accuracy: 0.8100\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.8129 - accuracy: 0.8230 - val_loss: 0.9379 - val_accuracy: 0.8100\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7899 - accuracy: 0.8270 - val_loss: 0.9165 - val_accuracy: 0.8100\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7776 - accuracy: 0.8260 - val_loss: 0.8963 - val_accuracy: 0.8150\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7504 - accuracy: 0.8340 - val_loss: 0.8775 - val_accuracy: 0.8150\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.7335 - accuracy: 0.8385 - val_loss: 0.8606 - val_accuracy: 0.8150\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7207 - accuracy: 0.8425 - val_loss: 0.8435 - val_accuracy: 0.8150\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.7006 - accuracy: 0.8455 - val_loss: 0.8277 - val_accuracy: 0.8200\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6883 - accuracy: 0.8460 - val_loss: 0.8131 - val_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.6782 - accuracy: 0.8485 - val_loss: 0.7990 - val_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6637 - accuracy: 0.8505 - val_loss: 0.7856 - val_accuracy: 0.8200\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6475 - accuracy: 0.8525 - val_loss: 0.7727 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6399 - accuracy: 0.8495 - val_loss: 0.7605 - val_accuracy: 0.8250\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.6274 - accuracy: 0.8520 - val_loss: 0.7486 - val_accuracy: 0.8200\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6159 - accuracy: 0.8610 - val_loss: 0.7384 - val_accuracy: 0.8350\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.6004 - accuracy: 0.8560 - val_loss: 0.7272 - val_accuracy: 0.8350\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5955 - accuracy: 0.8595 - val_loss: 0.7177 - val_accuracy: 0.8350\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5833 - accuracy: 0.8615 - val_loss: 0.7086 - val_accuracy: 0.8450\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5765 - accuracy: 0.8650 - val_loss: 0.7002 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5716 - accuracy: 0.8610 - val_loss: 0.6910 - val_accuracy: 0.8550\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5608 - accuracy: 0.8615 - val_loss: 0.6826 - val_accuracy: 0.8550\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5532 - accuracy: 0.8665 - val_loss: 0.6748 - val_accuracy: 0.8600\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5461 - accuracy: 0.8735 - val_loss: 0.6668 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5403 - accuracy: 0.8735 - val_loss: 0.6598 - val_accuracy: 0.8600\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5351 - accuracy: 0.8655 - val_loss: 0.6525 - val_accuracy: 0.8650\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5261 - accuracy: 0.8710 - val_loss: 0.6461 - val_accuracy: 0.8650\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 26ms/step - loss: 0.5211 - accuracy: 0.8760 - val_loss: 0.6399 - val_accuracy: 0.8650\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5126 - accuracy: 0.8770 - val_loss: 0.6337 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.5071 - accuracy: 0.8735 - val_loss: 0.6277 - val_accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4972 - accuracy: 0.8825 - val_loss: 0.6217 - val_accuracy: 0.8750\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 27ms/step - loss: 0.4977 - accuracy: 0.8775 - val_loss: 0.6169 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4934 - accuracy: 0.8745 - val_loss: 0.6114 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4835 - accuracy: 0.8800 - val_loss: 0.6056 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4818 - accuracy: 0.8785 - val_loss: 0.5998 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4735 - accuracy: 0.8805 - val_loss: 0.5962 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4694 - accuracy: 0.8820 - val_loss: 0.5916 - val_accuracy: 0.8800\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4666 - accuracy: 0.8800 - val_loss: 0.5873 - val_accuracy: 0.8800\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4611 - accuracy: 0.8845 - val_loss: 0.5818 - val_accuracy: 0.8800\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 0.4593 - accuracy: 0.8835 - val_loss: 0.5779 - val_accuracy: 0.8800\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4566 - accuracy: 0.8825 - val_loss: 0.5737 - val_accuracy: 0.8800\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 4s 59ms/step - loss: 0.4492 - accuracy: 0.8865 - val_loss: 0.5702 - val_accuracy: 0.8800\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.4453 - accuracy: 0.8855 - val_loss: 0.5668 - val_accuracy: 0.8800\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4407 - accuracy: 0.8890 - val_loss: 0.5629 - val_accuracy: 0.8800\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.4345 - accuracy: 0.8890 - val_loss: 0.5589 - val_accuracy: 0.8850\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4363 - accuracy: 0.8920 - val_loss: 0.5556 - val_accuracy: 0.8850\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.4332 - accuracy: 0.8920 - val_loss: 0.5523 - val_accuracy: 0.8850\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4295 - accuracy: 0.8910 - val_loss: 0.5487 - val_accuracy: 0.8850\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4260 - accuracy: 0.8905 - val_loss: 0.5453 - val_accuracy: 0.8850\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4231 - accuracy: 0.8980 - val_loss: 0.5418 - val_accuracy: 0.8850\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4169 - accuracy: 0.8950 - val_loss: 0.5390 - val_accuracy: 0.8850\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4133 - accuracy: 0.8885 - val_loss: 0.5358 - val_accuracy: 0.8850\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 3s 41ms/step - loss: 0.4141 - accuracy: 0.8945 - val_loss: 0.5332 - val_accuracy: 0.8850\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4082 - accuracy: 0.8920 - val_loss: 0.5305 - val_accuracy: 0.8850\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4076 - accuracy: 0.8920 - val_loss: 0.5282 - val_accuracy: 0.8850\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.4049 - accuracy: 0.8970 - val_loss: 0.5256 - val_accuracy: 0.8850\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3998 - accuracy: 0.8995 - val_loss: 0.5226 - val_accuracy: 0.8850\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3983 - accuracy: 0.9005 - val_loss: 0.5202 - val_accuracy: 0.8850\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3976 - accuracy: 0.8965 - val_loss: 0.5171 - val_accuracy: 0.8850\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3917 - accuracy: 0.9010 - val_loss: 0.5152 - val_accuracy: 0.8850\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3924 - accuracy: 0.9000 - val_loss: 0.5131 - val_accuracy: 0.8850\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3901 - accuracy: 0.9005 - val_loss: 0.5107 - val_accuracy: 0.8850\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3846 - accuracy: 0.8970 - val_loss: 0.5085 - val_accuracy: 0.8850\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3854 - accuracy: 0.8960 - val_loss: 0.5064 - val_accuracy: 0.8850\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3812 - accuracy: 0.9045 - val_loss: 0.5041 - val_accuracy: 0.8850\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3822 - accuracy: 0.9000 - val_loss: 0.5011 - val_accuracy: 0.8850\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3777 - accuracy: 0.9035 - val_loss: 0.4991 - val_accuracy: 0.8850\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3799 - accuracy: 0.9040 - val_loss: 0.4971 - val_accuracy: 0.8850\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3759 - accuracy: 0.8985 - val_loss: 0.4957 - val_accuracy: 0.8850\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3671 - accuracy: 0.9095 - val_loss: 0.4937 - val_accuracy: 0.8850\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3690 - accuracy: 0.9015 - val_loss: 0.4919 - val_accuracy: 0.8850\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3681 - accuracy: 0.9090 - val_loss: 0.4899 - val_accuracy: 0.8850\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 29ms/step - loss: 0.3690 - accuracy: 0.9040 - val_loss: 0.4879 - val_accuracy: 0.8850\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3615 - accuracy: 0.9090 - val_loss: 0.4865 - val_accuracy: 0.8950\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 28ms/step - loss: 0.3627 - accuracy: 0.9075 - val_loss: 0.4850 - val_accuracy: 0.8900\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3587 - accuracy: 0.9075 - val_loss: 0.4834 - val_accuracy: 0.8900\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3568 - accuracy: 0.9105 - val_loss: 0.4816 - val_accuracy: 0.8950\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3556 - accuracy: 0.9040 - val_loss: 0.4808 - val_accuracy: 0.8900\n",
            "Dropout model Test loss: 0.446363627910614\n",
            "Dropout model Test accuracy: 0.878600001335144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEyhMagnGq8F"
      },
      "source": [
        "##4.3 BatchNormalization\n",
        "BatchNormalization(BN)은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 BN layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 BN model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIBMssiBJaB",
        "outputId": "768ee7cf-d6bc-46da-85f8-de8b6d6242da"
      },
      "source": [
        "bn_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "bn_model.add(keras.Input(shape=(28,28,)))\n",
        "bn_model.add(Flatten())\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation(\"relu\"))\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation(\"relu\"))\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation(\"relu\"))\n",
        "bn_model.add(Flatten())\n",
        "bn_model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "bn_model.build()\n",
        "### END CODE HERE ###\n",
        "bn_model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_20 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1024)             4096      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_21 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izQBrVW6KYr3"
      },
      "source": [
        "BN Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2oakRaKZ4y",
        "outputId": "62b49927-b648-4e76-a738-bfbd9caac79c"
      },
      "source": [
        "bn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "bn_model_history=bn_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = bn_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('BN model Test loss:', score[0])\n",
        "print('BN model Test accuracy:', score[1])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 3s 33ms/step - loss: 2.6920 - accuracy: 0.1260 - val_loss: 2.2818 - val_accuracy: 0.1350\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 2.4796 - accuracy: 0.1625 - val_loss: 2.2561 - val_accuracy: 0.1600\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 2.3129 - accuracy: 0.2125 - val_loss: 2.1955 - val_accuracy: 0.2150\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 2.1251 - accuracy: 0.2575 - val_loss: 2.0929 - val_accuracy: 0.2550\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.9957 - accuracy: 0.3150 - val_loss: 1.9780 - val_accuracy: 0.3000\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.8557 - accuracy: 0.3735 - val_loss: 1.8662 - val_accuracy: 0.3850\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.7508 - accuracy: 0.4055 - val_loss: 1.7612 - val_accuracy: 0.4400\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.6506 - accuracy: 0.4520 - val_loss: 1.6667 - val_accuracy: 0.4500\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.5450 - accuracy: 0.4950 - val_loss: 1.5824 - val_accuracy: 0.4850\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.4629 - accuracy: 0.5310 - val_loss: 1.5058 - val_accuracy: 0.5100\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.4061 - accuracy: 0.5590 - val_loss: 1.4368 - val_accuracy: 0.5600\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.3113 - accuracy: 0.5815 - val_loss: 1.3740 - val_accuracy: 0.5600\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.2481 - accuracy: 0.6100 - val_loss: 1.3123 - val_accuracy: 0.5950\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.1863 - accuracy: 0.6370 - val_loss: 1.2580 - val_accuracy: 0.6000\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.1496 - accuracy: 0.6430 - val_loss: 1.2087 - val_accuracy: 0.6150\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.0797 - accuracy: 0.6790 - val_loss: 1.1594 - val_accuracy: 0.6250\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 1.0380 - accuracy: 0.6905 - val_loss: 1.1193 - val_accuracy: 0.6300\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.9978 - accuracy: 0.7035 - val_loss: 1.0797 - val_accuracy: 0.6600\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.9607 - accuracy: 0.7195 - val_loss: 1.0414 - val_accuracy: 0.6850\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.9311 - accuracy: 0.7305 - val_loss: 1.0055 - val_accuracy: 0.7050\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.8835 - accuracy: 0.7435 - val_loss: 0.9730 - val_accuracy: 0.7100\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.8476 - accuracy: 0.7680 - val_loss: 0.9418 - val_accuracy: 0.7200\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.8338 - accuracy: 0.7695 - val_loss: 0.9118 - val_accuracy: 0.7350\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.8029 - accuracy: 0.7800 - val_loss: 0.8849 - val_accuracy: 0.7450\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.7755 - accuracy: 0.7900 - val_loss: 0.8597 - val_accuracy: 0.7600\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.7493 - accuracy: 0.8055 - val_loss: 0.8362 - val_accuracy: 0.7650\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.7224 - accuracy: 0.8175 - val_loss: 0.8144 - val_accuracy: 0.7650\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6806 - accuracy: 0.8170 - val_loss: 0.7944 - val_accuracy: 0.7650\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6730 - accuracy: 0.8170 - val_loss: 0.7737 - val_accuracy: 0.7750\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6510 - accuracy: 0.8390 - val_loss: 0.7537 - val_accuracy: 0.7800\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6352 - accuracy: 0.8325 - val_loss: 0.7361 - val_accuracy: 0.7900\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6066 - accuracy: 0.8525 - val_loss: 0.7194 - val_accuracy: 0.7900\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6092 - accuracy: 0.8435 - val_loss: 0.7017 - val_accuracy: 0.7850\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5713 - accuracy: 0.8590 - val_loss: 0.6869 - val_accuracy: 0.7900\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5763 - accuracy: 0.8475 - val_loss: 0.6725 - val_accuracy: 0.8000\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5487 - accuracy: 0.8620 - val_loss: 0.6604 - val_accuracy: 0.8050\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5383 - accuracy: 0.8665 - val_loss: 0.6470 - val_accuracy: 0.8050\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5260 - accuracy: 0.8695 - val_loss: 0.6357 - val_accuracy: 0.8200\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5192 - accuracy: 0.8720 - val_loss: 0.6250 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4970 - accuracy: 0.8855 - val_loss: 0.6129 - val_accuracy: 0.8300\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4989 - accuracy: 0.8775 - val_loss: 0.6011 - val_accuracy: 0.8300\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4787 - accuracy: 0.8890 - val_loss: 0.5913 - val_accuracy: 0.8300\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4703 - accuracy: 0.8840 - val_loss: 0.5815 - val_accuracy: 0.8350\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4461 - accuracy: 0.8960 - val_loss: 0.5732 - val_accuracy: 0.8400\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4594 - accuracy: 0.8900 - val_loss: 0.5639 - val_accuracy: 0.8450\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4354 - accuracy: 0.8990 - val_loss: 0.5557 - val_accuracy: 0.8450\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4349 - accuracy: 0.8945 - val_loss: 0.5474 - val_accuracy: 0.8550\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4330 - accuracy: 0.8980 - val_loss: 0.5404 - val_accuracy: 0.8550\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4267 - accuracy: 0.9030 - val_loss: 0.5324 - val_accuracy: 0.8650\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4131 - accuracy: 0.8980 - val_loss: 0.5248 - val_accuracy: 0.8700\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4036 - accuracy: 0.9125 - val_loss: 0.5184 - val_accuracy: 0.8650\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3903 - accuracy: 0.9120 - val_loss: 0.5113 - val_accuracy: 0.8700\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3834 - accuracy: 0.9105 - val_loss: 0.5044 - val_accuracy: 0.8700\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3891 - accuracy: 0.9125 - val_loss: 0.4991 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3674 - accuracy: 0.9185 - val_loss: 0.4919 - val_accuracy: 0.8800\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3693 - accuracy: 0.9240 - val_loss: 0.4863 - val_accuracy: 0.8850\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3530 - accuracy: 0.9210 - val_loss: 0.4806 - val_accuracy: 0.8850\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3502 - accuracy: 0.9315 - val_loss: 0.4767 - val_accuracy: 0.8900\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3395 - accuracy: 0.9235 - val_loss: 0.4712 - val_accuracy: 0.8900\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.3309 - accuracy: 0.9285 - val_loss: 0.4671 - val_accuracy: 0.8900\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3390 - accuracy: 0.9265 - val_loss: 0.4610 - val_accuracy: 0.8900\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3272 - accuracy: 0.9295 - val_loss: 0.4555 - val_accuracy: 0.8900\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3261 - accuracy: 0.9285 - val_loss: 0.4520 - val_accuracy: 0.8900\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3231 - accuracy: 0.9310 - val_loss: 0.4481 - val_accuracy: 0.8900\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3180 - accuracy: 0.9270 - val_loss: 0.4435 - val_accuracy: 0.8950\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3063 - accuracy: 0.9400 - val_loss: 0.4395 - val_accuracy: 0.8950\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3114 - accuracy: 0.9270 - val_loss: 0.4348 - val_accuracy: 0.8950\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3080 - accuracy: 0.9385 - val_loss: 0.4312 - val_accuracy: 0.8950\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3048 - accuracy: 0.9345 - val_loss: 0.4270 - val_accuracy: 0.8950\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2920 - accuracy: 0.9425 - val_loss: 0.4237 - val_accuracy: 0.9050\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2915 - accuracy: 0.9390 - val_loss: 0.4197 - val_accuracy: 0.9050\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2785 - accuracy: 0.9475 - val_loss: 0.4166 - val_accuracy: 0.9100\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2788 - accuracy: 0.9405 - val_loss: 0.4128 - val_accuracy: 0.9100\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2714 - accuracy: 0.9495 - val_loss: 0.4093 - val_accuracy: 0.9050\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2721 - accuracy: 0.9460 - val_loss: 0.4062 - val_accuracy: 0.9050\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2676 - accuracy: 0.9450 - val_loss: 0.4039 - val_accuracy: 0.9050\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2616 - accuracy: 0.9510 - val_loss: 0.4017 - val_accuracy: 0.9050\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2561 - accuracy: 0.9485 - val_loss: 0.3989 - val_accuracy: 0.9050\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2575 - accuracy: 0.9480 - val_loss: 0.3954 - val_accuracy: 0.9050\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2524 - accuracy: 0.9510 - val_loss: 0.3927 - val_accuracy: 0.9050\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2547 - accuracy: 0.9450 - val_loss: 0.3899 - val_accuracy: 0.9100\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2476 - accuracy: 0.9480 - val_loss: 0.3859 - val_accuracy: 0.9100\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2425 - accuracy: 0.9515 - val_loss: 0.3841 - val_accuracy: 0.9100\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2518 - accuracy: 0.9485 - val_loss: 0.3818 - val_accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2356 - accuracy: 0.9540 - val_loss: 0.3786 - val_accuracy: 0.9100\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2320 - accuracy: 0.9590 - val_loss: 0.3764 - val_accuracy: 0.9100\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2372 - accuracy: 0.9520 - val_loss: 0.3736 - val_accuracy: 0.9150\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2314 - accuracy: 0.9565 - val_loss: 0.3722 - val_accuracy: 0.9150\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2195 - accuracy: 0.9580 - val_loss: 0.3703 - val_accuracy: 0.9150\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2181 - accuracy: 0.9635 - val_loss: 0.3686 - val_accuracy: 0.9150\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2201 - accuracy: 0.9580 - val_loss: 0.3662 - val_accuracy: 0.9150\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2208 - accuracy: 0.9560 - val_loss: 0.3631 - val_accuracy: 0.9150\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2071 - accuracy: 0.9615 - val_loss: 0.3617 - val_accuracy: 0.9150\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2093 - accuracy: 0.9640 - val_loss: 0.3609 - val_accuracy: 0.9150\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2120 - accuracy: 0.9565 - val_loss: 0.3576 - val_accuracy: 0.9150\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1960 - accuracy: 0.9670 - val_loss: 0.3563 - val_accuracy: 0.9150\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1992 - accuracy: 0.9665 - val_loss: 0.3547 - val_accuracy: 0.9150\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1999 - accuracy: 0.9650 - val_loss: 0.3520 - val_accuracy: 0.9150\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1995 - accuracy: 0.9660 - val_loss: 0.3499 - val_accuracy: 0.9150\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.1952 - accuracy: 0.9660 - val_loss: 0.3482 - val_accuracy: 0.9150\n",
            "BN model Test loss: 0.37040019035339355\n",
            "BN model Test accuracy: 0.8921999931335449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fDPwaFsJ47I"
      },
      "source": [
        "##4.4 Final Model\n",
        "지금까지 썼던 Regularization 기법들을 종합선물세트로 적용해 봅시다. 다음과 같은 Final model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8ppnPcELADo",
        "outputId": "61db3020-6689-4670-a7f0-87011c6c4572"
      },
      "source": [
        "final_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "final_model.add(keras.Input(shape=(28,28,)))\n",
        "final_model.add(Flatten())\n",
        "final_model.add(Dense(1024))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Activation(\"relu\"))\n",
        "final_model.add(Dense(1024))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Activation(\"relu\"))\n",
        "final_model.add(Dense(1024))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Activation(\"relu\"))\n",
        "final_model.add(Dropout(0.2))\n",
        "final_model.add(Flatten())\n",
        "final_model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "final_model.build()\n",
        "### END CODE HERE ###\n",
        "final_model.summary()\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_22 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_23 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocok8VnoLcb4"
      },
      "source": [
        "Final Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-bFRiHtLdVe",
        "outputId": "38aa93b3-c130-4a87-e4c6-77ce559cff9f"
      },
      "source": [
        "final_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "final_model_history=final_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = final_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Final model Test loss:', score[0])\n",
        "print('Final model Test accuracy:', score[1])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.6894 - accuracy: 0.1070 - val_loss: 2.2638 - val_accuracy: 0.1750\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.4844 - accuracy: 0.1515 - val_loss: 2.2048 - val_accuracy: 0.2400\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 2.3233 - accuracy: 0.1915 - val_loss: 2.1087 - val_accuracy: 0.2850\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 2.1715 - accuracy: 0.2430 - val_loss: 1.9868 - val_accuracy: 0.3400\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 2.0578 - accuracy: 0.2850 - val_loss: 1.8638 - val_accuracy: 0.4200\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.9298 - accuracy: 0.3255 - val_loss: 1.7564 - val_accuracy: 0.4350\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.7996 - accuracy: 0.3860 - val_loss: 1.6593 - val_accuracy: 0.4750\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.7136 - accuracy: 0.4200 - val_loss: 1.5741 - val_accuracy: 0.5100\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.6474 - accuracy: 0.4520 - val_loss: 1.4966 - val_accuracy: 0.5400\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.5495 - accuracy: 0.4915 - val_loss: 1.4259 - val_accuracy: 0.5650\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.4801 - accuracy: 0.5290 - val_loss: 1.3631 - val_accuracy: 0.5950\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.4147 - accuracy: 0.5515 - val_loss: 1.3081 - val_accuracy: 0.6150\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.3451 - accuracy: 0.5695 - val_loss: 1.2541 - val_accuracy: 0.6300\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 1.2732 - accuracy: 0.6155 - val_loss: 1.2038 - val_accuracy: 0.6550\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.2312 - accuracy: 0.6225 - val_loss: 1.1592 - val_accuracy: 0.6750\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.1553 - accuracy: 0.6520 - val_loss: 1.1182 - val_accuracy: 0.6800\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.1137 - accuracy: 0.6675 - val_loss: 1.0783 - val_accuracy: 0.6950\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.0905 - accuracy: 0.6890 - val_loss: 1.0417 - val_accuracy: 0.7150\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.0499 - accuracy: 0.6850 - val_loss: 1.0079 - val_accuracy: 0.7300\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.9998 - accuracy: 0.7140 - val_loss: 0.9772 - val_accuracy: 0.7500\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.9607 - accuracy: 0.7310 - val_loss: 0.9465 - val_accuracy: 0.7650\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.9250 - accuracy: 0.7460 - val_loss: 0.9198 - val_accuracy: 0.7650\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.9181 - accuracy: 0.7485 - val_loss: 0.8926 - val_accuracy: 0.7650\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.8879 - accuracy: 0.7470 - val_loss: 0.8696 - val_accuracy: 0.7700\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8391 - accuracy: 0.7740 - val_loss: 0.8475 - val_accuracy: 0.7900\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.8220 - accuracy: 0.7710 - val_loss: 0.8252 - val_accuracy: 0.8000\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.7975 - accuracy: 0.7810 - val_loss: 0.8065 - val_accuracy: 0.8150\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.7801 - accuracy: 0.7735 - val_loss: 0.7878 - val_accuracy: 0.8200\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.7598 - accuracy: 0.7960 - val_loss: 0.7675 - val_accuracy: 0.8200\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.7411 - accuracy: 0.7985 - val_loss: 0.7507 - val_accuracy: 0.8200\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.7212 - accuracy: 0.8055 - val_loss: 0.7350 - val_accuracy: 0.8250\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6989 - accuracy: 0.8140 - val_loss: 0.7209 - val_accuracy: 0.8250\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6856 - accuracy: 0.8195 - val_loss: 0.7061 - val_accuracy: 0.8250\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6616 - accuracy: 0.8235 - val_loss: 0.6927 - val_accuracy: 0.8250\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6635 - accuracy: 0.8195 - val_loss: 0.6789 - val_accuracy: 0.8250\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.6458 - accuracy: 0.8225 - val_loss: 0.6664 - val_accuracy: 0.8300\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6064 - accuracy: 0.8500 - val_loss: 0.6543 - val_accuracy: 0.8450\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5951 - accuracy: 0.8455 - val_loss: 0.6434 - val_accuracy: 0.8450\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6030 - accuracy: 0.8415 - val_loss: 0.6332 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5626 - accuracy: 0.8520 - val_loss: 0.6234 - val_accuracy: 0.8500\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5722 - accuracy: 0.8505 - val_loss: 0.6130 - val_accuracy: 0.8500\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5609 - accuracy: 0.8660 - val_loss: 0.6036 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5406 - accuracy: 0.8640 - val_loss: 0.5955 - val_accuracy: 0.8500\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5302 - accuracy: 0.8620 - val_loss: 0.5868 - val_accuracy: 0.8550\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5227 - accuracy: 0.8655 - val_loss: 0.5784 - val_accuracy: 0.8550\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5200 - accuracy: 0.8660 - val_loss: 0.5697 - val_accuracy: 0.8550\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.5256 - accuracy: 0.8660 - val_loss: 0.5629 - val_accuracy: 0.8550\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.5011 - accuracy: 0.8720 - val_loss: 0.5558 - val_accuracy: 0.8550\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4781 - accuracy: 0.8810 - val_loss: 0.5486 - val_accuracy: 0.8550\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4807 - accuracy: 0.8805 - val_loss: 0.5427 - val_accuracy: 0.8550\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4756 - accuracy: 0.8800 - val_loss: 0.5361 - val_accuracy: 0.8550\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4691 - accuracy: 0.8800 - val_loss: 0.5300 - val_accuracy: 0.8600\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4617 - accuracy: 0.8890 - val_loss: 0.5229 - val_accuracy: 0.8650\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4548 - accuracy: 0.8875 - val_loss: 0.5177 - val_accuracy: 0.8650\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4577 - accuracy: 0.8850 - val_loss: 0.5119 - val_accuracy: 0.8650\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4552 - accuracy: 0.8785 - val_loss: 0.5067 - val_accuracy: 0.8650\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4304 - accuracy: 0.8915 - val_loss: 0.5019 - val_accuracy: 0.8650\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4265 - accuracy: 0.8895 - val_loss: 0.4968 - val_accuracy: 0.8650\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4212 - accuracy: 0.8945 - val_loss: 0.4909 - val_accuracy: 0.8650\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4078 - accuracy: 0.9050 - val_loss: 0.4864 - val_accuracy: 0.8700\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4147 - accuracy: 0.8935 - val_loss: 0.4825 - val_accuracy: 0.8650\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.4058 - accuracy: 0.8975 - val_loss: 0.4781 - val_accuracy: 0.8700\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3947 - accuracy: 0.9040 - val_loss: 0.4741 - val_accuracy: 0.8700\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3917 - accuracy: 0.9070 - val_loss: 0.4692 - val_accuracy: 0.8750\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3810 - accuracy: 0.9075 - val_loss: 0.4656 - val_accuracy: 0.8750\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3715 - accuracy: 0.9085 - val_loss: 0.4606 - val_accuracy: 0.8800\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3833 - accuracy: 0.8955 - val_loss: 0.4573 - val_accuracy: 0.8750\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3656 - accuracy: 0.9125 - val_loss: 0.4536 - val_accuracy: 0.8800\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3628 - accuracy: 0.9195 - val_loss: 0.4490 - val_accuracy: 0.8800\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3613 - accuracy: 0.9100 - val_loss: 0.4461 - val_accuracy: 0.8800\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3480 - accuracy: 0.9110 - val_loss: 0.4421 - val_accuracy: 0.8800\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3346 - accuracy: 0.9280 - val_loss: 0.4386 - val_accuracy: 0.8800\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3438 - accuracy: 0.9215 - val_loss: 0.4355 - val_accuracy: 0.8800\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3516 - accuracy: 0.9155 - val_loss: 0.4309 - val_accuracy: 0.8900\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3509 - accuracy: 0.9105 - val_loss: 0.4273 - val_accuracy: 0.8900\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3356 - accuracy: 0.9215 - val_loss: 0.4251 - val_accuracy: 0.8900\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3309 - accuracy: 0.9235 - val_loss: 0.4221 - val_accuracy: 0.8900\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3115 - accuracy: 0.9285 - val_loss: 0.4193 - val_accuracy: 0.8900\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3215 - accuracy: 0.9255 - val_loss: 0.4179 - val_accuracy: 0.8900\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3218 - accuracy: 0.9300 - val_loss: 0.4162 - val_accuracy: 0.8900\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3182 - accuracy: 0.9225 - val_loss: 0.4128 - val_accuracy: 0.8900\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3003 - accuracy: 0.9325 - val_loss: 0.4096 - val_accuracy: 0.8900\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.3110 - accuracy: 0.9285 - val_loss: 0.4064 - val_accuracy: 0.8900\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2937 - accuracy: 0.9315 - val_loss: 0.4037 - val_accuracy: 0.8900\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.3002 - accuracy: 0.9315 - val_loss: 0.4025 - val_accuracy: 0.8900\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2867 - accuracy: 0.9405 - val_loss: 0.4001 - val_accuracy: 0.8950\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2843 - accuracy: 0.9415 - val_loss: 0.3978 - val_accuracy: 0.9000\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.2857 - accuracy: 0.9320 - val_loss: 0.3961 - val_accuracy: 0.8950\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2758 - accuracy: 0.9385 - val_loss: 0.3935 - val_accuracy: 0.9000\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2734 - accuracy: 0.9345 - val_loss: 0.3911 - val_accuracy: 0.9000\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2747 - accuracy: 0.9380 - val_loss: 0.3889 - val_accuracy: 0.9000\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2760 - accuracy: 0.9345 - val_loss: 0.3864 - val_accuracy: 0.9000\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2651 - accuracy: 0.9340 - val_loss: 0.3846 - val_accuracy: 0.9000\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2692 - accuracy: 0.9345 - val_loss: 0.3832 - val_accuracy: 0.9000\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 30ms/step - loss: 0.2577 - accuracy: 0.9420 - val_loss: 0.3809 - val_accuracy: 0.9000\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2560 - accuracy: 0.9455 - val_loss: 0.3784 - val_accuracy: 0.9000\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2567 - accuracy: 0.9435 - val_loss: 0.3762 - val_accuracy: 0.9000\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2567 - accuracy: 0.9440 - val_loss: 0.3747 - val_accuracy: 0.9000\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2420 - accuracy: 0.9450 - val_loss: 0.3730 - val_accuracy: 0.9000\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 31ms/step - loss: 0.2604 - accuracy: 0.9440 - val_loss: 0.3711 - val_accuracy: 0.9000\n",
            "Final model Test loss: 0.3784692585468292\n",
            "Final model Test accuracy: 0.8891000151634216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPR3nEMAL4Yd"
      },
      "source": [
        "(optional) Training data가 늘어나면 regularization 효과가 나는 것을 보였습니다. 쉽게 training data를 늘릴 수 있는 방법은 data augmentation 입니다. 이 방법은 기존 training data를 적절히 rotating, flipping, scaling, shifting 하여 training data 수를 늘리는 것입니다. data augmentation의 regularization 효과를 테스트해 보세요. 또한 뉴럴 네트워크의 노드 개수나 층수를 바꿔서 성능을 올려보는 것도 테스트해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn11ZtM7GmS-"
      },
      "source": [
        ""
      ]
    }
  ]
}